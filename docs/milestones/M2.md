# Milestone 2: Perception Pipeline

**Status:** Not Started
**Ref:** `docs/PROJECT.md` Section 7, Milestone 2

## Objective

Build the full perception pipeline from camera input to robot joint commands. After M2, running `python src/main.py --sim` opens the webcam and PyBullet side-by-side, and the simulated robot mirrors the user's arm movements in real-time.

## Dependencies

- M0 complete (venv, dependencies, MediaPipe model downloaded)
- M1 complete (`SimulatedRobot` works, URDF loads)
- Webcam available
- **Important:** Uses MediaPipe tasks API (`mediapipe.tasks.python.vision.PoseLandmarker`), NOT the legacy `mp.solutions.pose` API. See M0 notes.

## Out of Scope

- ESP32 serial communication (M3)
- `RealRobot` implementation (M3/M4)
- Custom pose model training
- Full-body tracking (upper body only: shoulders, elbows, wrists)

---

## Deliverables

| # | Artifact | Path | Description |
|---|----------|------|-------------|
| D1 | Camera module | `src/camera.py` | Frame capture with timestamps |
| D2 | Pose estimator | `src/pose_estimator.py` | MediaPipe tasks API wrapper, extracts 6 upper-body keypoints |
| D3 | Angle calculator | `src/angle_calculator.py` | Converts keypoints to 6 joint angles with EMA smoothing |
| D4 | Motion mapper | `src/motion_mapper.py` | Maps human angles to robot angles with mirroring, scaling, limits, dead zone |
| D5 | Main control loop | `src/main.py` | End-to-end pipeline: camera → pose → angles → mapper → robot |
| D6 | Pipeline test | `tests/test_pipeline.py` | Headless tests for angle calculation and motion mapping |

---

## Technical Spec

### S1: `src/camera.py`

```python
@dataclass
class Frame:
    image: np.ndarray       # BGR, shape (H, W, 3)
    timestamp: float        # time.time()
    frame_number: int

class Camera:
    def __init__(self, device_id: int = 0, width: int = 640, height: int = 480, fps: int = 30)
    def open(self) -> bool
    def read(self) -> Frame | None
    def release(self) -> None
    def is_opened(self) -> bool
```

- Uses `cv2.VideoCapture`
- Sets `CAP_PROP_FRAME_WIDTH`, `CAP_PROP_FRAME_HEIGHT`, `CAP_PROP_FPS` on open
- `read()` returns `None` on failure, increments `frame_count`
- `release()` calls `cap.release()`, sets `cap = None`

### S2: `src/pose_estimator.py`

Must use the MediaPipe tasks API, not the legacy solutions API.

```python
@dataclass
class Point3D:
    x: float            # normalized [0, 1] from pose_landmarks
    y: float
    z: float
    visibility: float
    # World coordinates (meters) from pose_world_landmarks
    world_x: float
    world_y: float
    world_z: float

@dataclass
class PoseResult:
    keypoints: dict[str, Point3D]   # 6 upper-body keypoints
    is_valid: bool                   # True if all required keypoints visible
    timestamp: float

class PoseEstimator:
    KEYPOINT_INDICES: dict[int, str] = {
        11: 'left_shoulder',
        12: 'right_shoulder',
        13: 'left_elbow',
        14: 'right_elbow',
        15: 'left_wrist',
        16: 'right_wrist',
    }

    def __init__(self, model_path: str, min_visibility: float = 0.5)
    def process(self, image: np.ndarray, timestamp: float) -> PoseResult
    def draw(self, image: np.ndarray, pose: PoseResult) -> np.ndarray
    def close(self) -> None
```

**Constructor:**
- Create `vision.PoseLandmarkerOptions` with `BaseOptions(model_asset_path=model_path)`, `num_poses=1`
- Create `vision.PoseLandmarker.create_from_options(options)`
- Store `min_visibility`

**`process(image, timestamp)`:**
- Convert BGR→RGB, wrap in `mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)`
- Call `landmarker.detect(mp_image)`
- If `result.pose_landmarks` is non-empty:
  - Extract 6 keypoints (indices 11-16) from `result.pose_landmarks[0]` (normalized coords)
  - Also extract from `result.pose_world_landmarks[0]` (world coords in meters) for 3D angle calculation
  - Set `is_valid = True` if all 6 keypoints have `visibility >= min_visibility`
- Return `PoseResult`

**`draw(image, pose)`:**
- Draw circles at keypoint positions (green if visibility > 0.8, orange otherwise)
- Draw bone lines: left_shoulder→left_elbow, left_elbow→left_wrist, right_shoulder→right_elbow, right_elbow→right_wrist, left_shoulder→right_shoulder
- Return annotated image copy

**`close()`:**
- Call `landmarker.close()`

### S3: `src/angle_calculator.py`

Converts 3D keypoint positions to 6 robot joint angles.

```python
@dataclass
class JointAngles:
    left_shoulder_pan: float
    left_shoulder_tilt: float
    left_elbow: float
    right_shoulder_pan: float
    right_shoulder_tilt: float
    right_elbow: float
    timestamp: float
    valid: np.ndarray       # shape (6,) bool

    def to_array(self) -> np.ndarray   # returns 6-element array

class AngleCalculator:
    def __init__(self, smoothing_factor: float = 0.3)
    def calculate(self, pose: PoseResult) -> JointAngles | None
    def reset(self) -> None
```

**Angle calculations** use world coordinates (`world_x`, `world_y`, `world_z`) from the pose result for accurate 3D angles.

**`_calc_shoulder_pan(shoulder, elbow, other_shoulder)`:**
- Project upper arm and torso direction onto horizontal (XZ) plane
- Compute signed angle between them using arccos + cross product for sign
- Positive = arm forward of torso line

**`_calc_shoulder_tilt(shoulder, elbow)`:**
- Compute angle of upper arm vector from horizontal
- Use `arctan2(-upper_arm_y, horizontal_distance)` (negative Y because MediaPipe Y points down)
- 0 = arm horizontal, positive = arm raised

**`_calc_elbow_angle(shoulder, elbow, wrist)`:**
- Angle between upper arm and forearm vectors
- Elbow flexion = `pi - angle_between_vectors`
- 0 = arm straight, positive = bent

**EMA smoothing:**
- `smoothed = alpha * previous + (1 - alpha) * current`
- `alpha = smoothing_factor` (0.3 default)
- Higher alpha = more smoothing (slower response)
- Applied independently to each of the 6 angles
- First frame: no smoothing applied

### S4: `src/motion_mapper.py`

Maps human joint angles to robot servo angles.

```python
@dataclass
class MappingConfig:
    mirror_mode: bool = True
    dead_zone: float = 0.05    # radians (~3 degrees)
    joint_limits: dict[str, tuple[float, float]]   # from URDF
    scale_factors: dict[str, float]                 # default 1.0

@dataclass
class ServoAngles:
    angles: np.ndarray      # shape (6,), radians
    timestamp: float

class MotionMapper:
    def __init__(self, config: MappingConfig = None)
    def map(self, human_angles: JointAngles) -> ServoAngles
    def reset(self) -> None
```

**`map(human_angles)` pipeline:**

1. **Mirror** (if `mirror_mode=True`): swap left↔right, negate pan angles
   ```
   robot_l_pan  = -human_r_pan
   robot_l_tilt =  human_r_tilt
   robot_l_elbow = human_r_elbow
   robot_r_pan  = -human_l_pan
   robot_r_tilt =  human_l_tilt
   robot_r_elbow = human_l_elbow
   ```

2. **Scale**: multiply each angle by its scale factor

3. **Clamp**: clip to joint limits
   ```
   shoulder_pan:  [-pi/2, pi/2]     (-90° to 90°)
   shoulder_tilt: [-pi/4, 3*pi/4]   (-45° to 135°)
   elbow:         [0, 3*pi/4]       (0° to 135°)
   ```

4. **Dead zone**: if `|new - prev| < dead_zone` for a joint, keep previous value

### S5: `src/main.py`

The main control loop that wires everything together.

**CLI arguments:**
- `--sim` — use PyBullet simulation (default: off)
- `--config PATH` — config file (default: `config/settings.yaml`)
- `--port PORT` — serial port override
- `--no-viz` — disable camera visualization window

**Pipeline per frame:**
```
camera.read() → pose_estimator.process() → angle_calculator.calculate() → motion_mapper.map() → robot.set_joint_positions()
```

**Additional behaviors:**
- If `--sim`: call `robot.step()` each iteration
- If not `--no-viz`: show camera feed with pose overlay, FPS counter, and current angles
- Print FPS stats to console every 5 seconds
- ESC or 'q' to quit
- Cleanup on exit: `robot.home()`, `robot.disconnect()`, `camera.release()`

**Config loading** from `config/settings.yaml`:
- `camera.device_id`, `camera.width`, `camera.height`, `camera.fps`
- `pose.model_path` (default: `data/pose_landmarker_lite.task`)
- `pose.min_visibility` (default: 0.5)
- `angles.smoothing_factor`
- `mapping.mirror_mode`, `mapping.dead_zone_deg`
- `simulation.urdf_path`
- `serial.port`, `serial.baud`

### S6: `tests/test_pipeline.py`

Headless tests that verify angle calculation and motion mapping without a camera.

**`test_elbow_angle_straight()`**
- Create synthetic keypoints with arm fully extended (shoulder, elbow, wrist collinear)
- Assert elbow angle ≈ 0

**`test_elbow_angle_bent()`**
- Create synthetic keypoints with arm bent at 90°
- Assert elbow angle ≈ pi/2

**`test_shoulder_tilt_horizontal()`**
- Create synthetic keypoints with arm pointing horizontally
- Assert shoulder tilt ≈ 0

**`test_shoulder_tilt_raised()`**
- Create synthetic keypoints with arm pointing straight up
- Assert shoulder tilt ≈ pi/2

**`test_mirror_mode()`**
- Create `JointAngles` with distinct left/right values
- Map with `mirror_mode=True`
- Assert left output came from right input and vice versa
- Assert pan angles are negated

**`test_joint_limits_clamping()`**
- Create `JointAngles` with values beyond limits
- Map through `MotionMapper`
- Assert all output values within joint limits

**`test_dead_zone()`**
- Map an initial `JointAngles`
- Map again with tiny change (< dead zone)
- Assert output unchanged
- Map again with large change (> dead zone)
- Assert output changed

**`test_smoothing()`**
- Feed a step change through `AngleCalculator` with known smoothing factor
- Verify output follows EMA formula

---

## Tasks

### Task 2.1: Implement `src/camera.py`

Replace stub with full implementation per S1.

**Produces:** `src/camera.py`
**Done when:** `from src.camera import Camera, Frame` succeeds

### Task 2.2: Implement `src/pose_estimator.py`

Replace stub with MediaPipe tasks API implementation per S2.

**Requires:** Task 2.1
**Produces:** `src/pose_estimator.py`
**Done when:** `from src.pose_estimator import PoseEstimator, PoseResult, Point3D` succeeds

### Task 2.3: Implement `src/angle_calculator.py`

Replace stub with full implementation per S3.

**Requires:** Task 2.2
**Produces:** `src/angle_calculator.py`
**Done when:** `from src.angle_calculator import AngleCalculator, JointAngles` succeeds

### Task 2.4: Implement `src/motion_mapper.py`

Replace stub with full implementation per S4.

**Requires:** Task 2.3
**Produces:** `src/motion_mapper.py`
**Done when:** `from src.motion_mapper import MotionMapper, MappingConfig, ServoAngles` succeeds

### Task 2.5: Update `config/settings.yaml`

Add `pose.model_path` and `pose.min_visibility` fields.

**Produces:** updated `config/settings.yaml`
**Done when:** Config includes all fields referenced by S5

### Task 2.6: Implement `src/main.py`

Replace stub with full control loop per S5.

**Requires:** Tasks 2.1–2.5, M1 (`SimulatedRobot`)
**Produces:** `src/main.py`
**Done when:** `python src/main.py --sim` starts without import errors

### Task 2.7: Create `tests/test_pipeline.py`

Write headless tests per S6.

**Requires:** Tasks 2.3, 2.4
**Produces:** `tests/test_pipeline.py`
**Done when:** `pytest tests/test_pipeline.py -v` passes all tests

### Task 2.8: Integration test — simulation mirroring

Run `python src/main.py --sim` and verify:
- Camera feed shows pose overlay
- PyBullet robot mirrors arm movements
- FPS displayed on camera window
- Quit with 'q' works cleanly

**Requires:** Task 2.6
**Done when:** Manual verification — robot mirrors user's punches

---

## Acceptance Criteria

- [ ] `python src/main.py --sim` opens webcam + PyBullet and robot mirrors user
- [ ] Pose overlay draws skeleton on camera feed
- [ ] FPS ≥ 15 Hz sustained
- [ ] Left jab by user → robot's right arm extends (mirror mode)
- [ ] Angles update smoothly (no visible jitter)
- [ ] Joint limits respected (robot never exceeds URDF limits)
- [ ] 'q' key quits cleanly (robot homes, windows close)
- [ ] `pytest tests/test_pipeline.py` passes all headless tests
